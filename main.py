# main.py โ Telegram bot with resilient Gemini model selection
import os
import telebot
from telebot import types
import google.generativeai as genai
import time

# ========= Env vars (set these in Railway Project Variables) =========
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
# Optional: force a specific model name (e.g. "models/text-bison-001")
GEMINI_MODEL_OVERRIDE = os.getenv("GEMINI_MODEL", "").strip()
# =====================================================================

if not TELEGRAM_TOKEN or not GEMINI_API_KEY:
    raise SystemExit("Missing TELEGRAM_TOKEN or GEMINI_API_KEY environment variables.")

# configure genai
genai.configure(api_key=GEMINI_API_KEY)

bot = telebot.TeleBot(TELEGRAM_TOKEN)

# Candidate fallback list (ูุญุงููุฉ ุฐููุฉ ููููุฏููุงุช ุงูุดุงุฆุนุฉ)
CANDIDATE_MODELS = [
    GEMINI_MODEL_OVERRIDE,
    "models/text-bison-001",
    "models/text-bison",
    "gemini-2.5-flash",
    "gemini-2.0-flash",
    "gemini-1.5-flash",
]

def discover_working_model():
    """
    1) ูุญุงูู ูุฑุงุกุฉ ูุงุฆูุฉ ุงูููุฏููุงุช ูู API ุฅู ุฃููู.
    2) ูุฌุฑุจ ููุฏููุงู ูู ูุงุฆูุฉ ุงููุฑุดุญูู ูููุจูู ุฃูู ููุฏูู ูุนูู.
    """
    tried = []
    # 1) ุญุงูู ูุฑุงุกุฉ list_models ูุชุนุทู ุชูููุญ ููููุฏููุงุช ุงููุชุงุญุฉ
    try:
        models_info = genai.list_models()
        # models_info ูุฏ ุชููู dict/listุ ูุญุงูู ุงุณุชุฎุฑุงุฌ ุฃุณูุงุก ููุฏููุงุช ุฅู ููุฌุฏุช
        available = []
        if isinstance(models_info, dict) and "models" in models_info:
            available = [m.get("name") or m.get("id") for m in models_info["models"] if m]
        elif isinstance(models_info, list):
            available = [m.get("name") or m.get("id") for m in models_info if isinstance(m, dict)]
        # ุถููู ุงููุชุงุญูู ุฅูู ูุงุฆูุฉ ุงููุฑุดุญูู ูุจู ุงูููู ุงูุงูุชุฑุงุถูุฉ
        for name in available:
            if name and name not in CANDIDATE_MODELS:
                CANDIDATE_MODELS.append(name)
    except Exception as e:
        print("Warning: list_models failed (will try fallbacks). Exception:", e)

    # 2) ุฌุฑูุจ ูู ููุฏูู ูู ุงููุฑุดุญูู ุนูููุงู ุจุฅุฑุณุงู ุทูุจ ุตุบูุฑ ููุชุฃูุฏ
    for m in CANDIDATE_MODELS:
        if not m:
            continue
        try:
            print(f"Trying model: {m}")
            model = genai.GenerativeModel(m)
            # ุงุฎุชุจุงุฑ ุจุณูุท ุฌุฏุงู ููุชุฃูุฏ ูู ุนูู generate_content
            test = model.generate_content("ุงุฎุชุจุงุฑ ุงูุชูุตูู. ุงุฌุจ ุจูููุฉ 'OK' ููุท.", max_output_tokens=10)
            text = getattr(test, "text", None)
            if text and "OK" in text:
                print("Selected working model:", m)
                return m
            # ุจุนุถ ุงูููุฏููุงุช ูุฏ ุชุฑุฌุน ุจููุฉ ูุฎุชููุฉ ููู ุจุฏูู ุงุณุชุซูุงุก => ุงุนุชุจุฑูุง ุตุงูุญุฉ
            print(f"Model {m} responded (accepting).")
            return m
        except Exception as e:
            tried.append((m, str(e)))
            print(f"Model {m} failed: {e}")
            # ุงูุงูุชุธุงุฑ ูููููุง ูุชุฌููุจ ูููุฏ ุงูุณุฑุนุฉ
            time.sleep(0.5)

    # ุฅู ูู ููุฌุญ ุดูุกุ ุฑูุน ุงูุฃุฎุทุงุก ููู Logs
    print("No candidate models worked. Tried:", tried)
    raise RuntimeError(f"No working Gemini model found. Tried: {tried}")

# ุงูุชุดุงู ุงูููุฏูู ุนูุฏ ุจุฏุก ุงูุชุดุบูู
try:
    WORKING_MODEL = discover_working_model()
except Exception as e:
    # ุงูุดู ุจุดูู ูุงุถุญ โ ุณูุชู ุชุณุฌููู ูู ููุบ Railway
    print("Fatal: cannot find working Gemini model:", e)
    WORKING_MODEL = None

def analyze_with_gemini(prompt):
    if not WORKING_MODEL:
        return "โ๏ธ ูู ูุชู ุงูุนุซูุฑ ุนูู ููุฏูู Gemini ุตุงูุญุ ุชุญูู ูู ููุงุชูุญ API ุฃู ุงูุณุนุฉ."
    try:
        model = genai.GenerativeModel(WORKING_MODEL)
        # generate_content ูุน ุฅุนุฏุงุฏุงุช ุขููุฉ
        resp = model.generate_content(
            prompt,
            temperature=0.3,
            max_output_tokens=700
        )
        # ุจุนุถ ูุณุฎ SDK ุชุฑุฌุน ุงููุต ูู resp.text
        text = getattr(resp, "text", None)
        if not text:
            # ุญุงูู ุงุณุชุฎุฑุงุฌ ุดูู ุขุฎุฑ ุฅุฐุง ููุฌุฏ
            text = str(resp)
        return text
    except Exception as e:
        # ุณุฌูู ุงูุงุณุชุซูุงุก ุงููุงูู ูู Logs (Railway Logs)
        print("Gemini call error:", type(e).__name__, str(e))
        return f"โ๏ธ ุฎุทุฃ ุนูุฏ ุงูุงุณุชุนูุงู ูู Gemini: {str(e)}"

# ---- prompt ููุญุณูู (ุงุณุชุนูููู ููุงูุจ) ----
PROMPT_TEMPLATE = """
ุฃูุช ูุญูู ุฑูุงุถู ูุญุชุฑู ูุชูุฑุณ. ุญูู ูุจุงุฑุงุฉ/ูุจุงุฑูุงุช ุจูุงุกู ุนูู ุงููุนุทูุงุช ุงูุชุงููุฉ:
- ุงุณุชุฎุฏู ุขุฎุฑ 6 ูุจุงุฑูุงุช ููู ูุฑูู (ูุชุงุฆุฌุ ุฃูุฏุงูุ ุฑููุงุช ุฑูููุฉ ุฅู ููุฌุฏุช).
- ูููู ุญุงูุฉ ุงููุฑูู ุงูููุณูุฉ (ูุซู ุชููุจุงุช ุงูุฃุฏุงุกุ ูุฒูุงุช ุงููุงุนุจููุ ุถุบุท ุงูุฌูููุฑ).
- ุฑูุฒ ุนูู ุณููููู ููุท: 'ุฑูููุงุช (corners)' ู'Double Chance (12)'.
- ุฃุนุทู ูุชูุฌุฉ ููุชุฑุญุฉ ูุงุญุฏุฉ ูุน ุฏุฑุฌุฉ ุซูุฉ (ุนุงููุฉ / ูุชูุณุทุฉ / ููุฎูุถุฉ).
- ูุฏูู ุณุจุจูุง ูุฎุชุตุฑูุง (3-5 ููุงุท) ูุนุชูุฏ ุนูู ุงูุฅุญุตุงุฆูุงุช ูุงูุชุญููู ุงูููุณู.
- ูู ููุฌุฒุงู ููุงุถุญุงู ุจุงูุนุฑุจูุฉ.

ุงููุทููุจ: {context}
"""

# ---- Telegram UI ----
def main_menu():
    kb = types.ReplyKeyboardMarkup(resize_keyboard=True)
    kb.add("๐ช๐บ ุงูุฏูุฑูุงุช ุงูู 5 ุงููุจุฑู", "๐ ุงูุญุตุงู ุงูุฃุณูุฏ")
    kb.add("๐ฅ ูุฑูุฉ ุงูููู")
    return kb

@bot.message_handler(commands=["start"])
def cmd_start(m):
    bot.send_message(m.chat.id, "โฝ ูุฑุญุจุงู โ ุงุฎุชุฑ ูุณู ุงูุชุญููู:", reply_markup=main_menu())

@bot.message_handler(func=lambda msg: True)
def handle_buttons(m):
    if m.text not in ["๐ช๐บ ุงูุฏูุฑูุงุช ุงูู 5 ุงููุจุฑู", "๐ ุงูุญุตุงู ุงูุฃุณูุฏ", "๐ฅ ูุฑูุฉ ุงูููู"]:
        bot.send_message(m.chat.id, "ุงุฎุชุฑ ูู ุงูุฃุฒุฑุงุฑ ุงูููุฌูุฏุฉ โฌ๏ธ", reply_markup=main_menu())
        return

    bot.send_message(m.chat.id, "๐ ุฌุงุฑู ุชุญุถูุฑ ุงูุชุญููู... ุงูุฑุฌุงุก ุงูุงูุชุธุงุฑ ููููุงู.")
    context = {
        "๐ช๐บ ุงูุฏูุฑูุงุช ุงูู 5 ุงููุจุฑู": "ุงูุฏูุฑูุงุช ุงูุฃูุฑูุจูุฉ ุงูุฎูุณ ุงููุจุฑู ุงูููู โ ุงุฎุชุฑ ูุจุงุฑุงุฉ ูุงุญุฏุฉ ุฃู ุนุทู ุงูููุงู ุงูุนุงู",
        "๐ ุงูุญุตุงู ุงูุฃุณูุฏ": "ุงุจุญุซ ุนู ูุจุงุฑุงุฉ ุจูุง ูููุฉ ุนุงููุฉ ููุฑูู ุบูุฑ ูุชููุน",
        "๐ฅ ูุฑูุฉ ุงูููู": "ุฃุนุทูู ูุฑูุฉ ุฑูุงู ุขููุฉ ููุจุงุฑุงุฉ ุงูููู (ุฑูููุงุช ุฃู Double Chance 12)"
    }.get(m.text, "ุชุญููู ุนุงู")

    prompt = PROMPT_TEMPLATE.format(context=context)
    res = analyze_with_gemini(prompt)
    bot.send_message(m.chat.id, res)

if __name__ == "__main__":
    print("Bot started. Working model:", WORKING_MODEL)
    bot.infinity_polling()
